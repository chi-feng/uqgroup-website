[
	{
		"id": "24",
		"title": "A priori testing of sparse adaptive polynomial chaos expansions using an ocean general circulation model database",
		"keywords": "",
		"authors": [
			"Winokur, J.",
			"Conrad, P.",
			"Sraj, I.",
			"Knio, O.",
			"Srinivasan, A.",
			"Thacker, W.C.",
			"Marzouk, Y.M.",
			"Iskandarani, M."
		],
		"abstract": "",
		"order": "25",
		"fulltext": "",
		"journal": "Submitted",
		"doi": "",
		"year": "2013",
		"arxiv": "",
		"volume": "",
		"thumbnail": "arxiv1.png",
		"number": "",
		"month": "0",
		"pages": "",
		"comments": "Submitted to Computational Geosciences"
	},
	{
		"id": "23",
		"title": "Gradient-based stochastic optimization methods in Bayesian experimental design",
		"keywords": "",
		"authors": [
			"Huan, X.",
			"Marzouk, Y.M."
		],
		"abstract": "Optimal experimental design (OED) seeks experiments expected to yield the most useful data for some purpose. In practical circumstances where experiments are time-consuming or resource-intensive, OED can yield enormous savings. We pursue OED for nonlinear systems from a Bayesian perspective, with the goal of choosing experiments that are optimal for parameter inference. Our objective in this context is the expected information gain in model parameters, which in general can only be estimated using Monte Carlo methods. Maximizing this objective thus becomes a stochastic optimization problem. \r\n\r\n<p>This paper develops gradient-based stochastic optimization methods for the design of experiments on a continuous parameter space. Given a Monte Carlo estimator of expected information gain, we use infinitesimal perturbation analysis to derive gradients of this estimator. We are then able to formulate two gradient-based stochastic optimization approaches: (i) Robbins-Monro stochastic approximation, and (ii) sample average approximation combined with a deterministic quasi-Newton method. A polynomial chaos approximation of the forward model accelerates objective and gradient evaluations in both cases. We discuss the implementation of these optimization methods, then conduct an empirical comparison of their performance. To demonstrate design in a nonlinear setting with partial differential equation forward models, we use the problem of sensor placement for source inversion. Numerical results yield useful guidelines on the choice of algorithm and sample sizes, assess the impact of estimator bias, and quantify tradeoffs of computational cost versus solution quality and robustness.",
		"order": "24",
		"fulltext": "http:\/\/arxiv.org\/abs\/1212.2228",
		"journal": "Submitted",
		"doi": "",
		"year": "2013",
		"arxiv": "http:\/\/arxiv.org\/abs\/1212.2228",
		"volume": "",
		"thumbnail": "arxiv1.png",
		"number": "",
		"month": "0",
		"pages": "",
		"comments": "Submitted to International Journal for Uncertainty Quantification"
	},
	{
		"id": "22",
		"title": "Adaptive Smolyak pseudospectral approximations",
		"keywords": "Smolyak algorithms, sparse grids, orthogonal polynomials, pseudospectral approximation, approximation theory, uncertainty quantification",
		"authors": [
			"Conrad, P.",
			"Marzouk, Y.M."
		],
		"abstract": "Polynomial approximations of computationally intensive models are central to uncertainty quantification. This paper describes an adaptive method for non-intrusive pseudospectral approximation, based on Smolyak's algorithm with generalized sparse grids. We rigorously develop and generalize the non-adaptive method proposed in [6], and compare it to a common alternative approach for using sparse grids to construct polynomial approximations, direct quadrature. Analysis of direct quadrature shows that O(1) errors are an intrinsic property of some configurations of the method, as a consequence of internal aliasing. We provide precise conditions, based on the chosen polynomial basis and quadrature rules, under which this aliasing error occurs. We then establish theoretical results on the accuracy of Smolyak pseudospectral approximation, and show that the Smolyak approximation avoids internal aliasing and makes far more effective use of sparse function evaluations. These results are applicable to broad choices of quadrature rule and generalized sparse grids. Exploiting this flexibility, we introduce a greedy heuristic for adaptive refinement of the pseudospectral approximation. We numerically demonstrate convergence of the algorithm on the Genz test functions, and illustrate the accuracy and efficiency of the adaptive approach on a realistic chemical kinetics problem.",
		"order": "23",
		"fulltext": "http:\/\/arxiv.org\/abs\/1209.1406",
		"journal": "Submitted",
		"doi": "",
		"year": "2013",
		"arxiv": "http:\/\/arxiv.org\/abs\/1209.1406",
		"volume": "",
		"thumbnail": "arxiv2.png",
		"number": "",
		"month": "0",
		"pages": "",
		"comments": "Submitted to SIAM Journal on Scientific Computing"
	},
	{
		"id": "17",
		"title": "Bayesian inverse problems with Monte Carlo forward models",
		"keywords": "linear transport; perturbation Monte Carlo; Bayesian; importance sampling; inverse problems; Markov chain Monte Carlo",
		"authors": [
			"Bal, G.",
			"Langmore, I.",
			"Marzouk, Y.M."
		],
		"abstract": "<p>The full application of Bayesian inference to inverse problems requires exploration of a posterior distribution that typically does not possess a standard form. In this context, Markov chain Monte Carlo (MCMC) methods are often used. These methods require many evaluations of a computationally intensive forward model to produce the equivalent of one independent sample from the posterior. We consider applications in which approximate forward models at multiple resolution levels are available, each endowed with a probabilistic error estimate. These situations occur, for example, when the forward model involves Monte Carlo integration. We present a novel MCMC method called $MC^3$ that uses low-resolution forward models to approximate draws from a posterior distribution built with the high-resolution forward model. The acceptance ratio is estimated with some statistical error; then a confidence interval for the true acceptance ratio is found, and acceptance is performed correctly with some confidence. The high-resolution models are rarely run and a significant speed up is achieved.<\/p><p>Our multiple-resolution forward models themselves are built around a new importance sampling scheme that allows Monte Carlo forward models to be used efficiently in inverse problems. The method is used to solve an inverse transport problem that finds applications in atmospheric remote sensing. We present a path-recycling methodology to efficiently vary parameters in the transport equation. The forward transport equation is solved by a Monte Carlo method that is amenable to the use of $MC^3$ to solve the inverse transport problem using a Bayesian formalism.<\/p>",
		"order": "22",
		"fulltext": "http:\/\/www.columbia.edu\/~gb2030\/PAPERS\/mc3.pdf",
		"journal": "Inverse Problems and Imaging",
		"doi": "10.3934\/ipi.2013.7.81 ",
		"year": "2013",
		"arxiv": "",
		"volume": "7",
		"thumbnail": "ipi.png",
		"number": "1",
		"month": "0",
		"pages": "81\u2013105",
		"comments": ""
	},
	{
		"id": "19",
		"title": "Simulation-based optimal Bayesian experimental design for nonlinear systems",
		"keywords": "Uncertainty quantification; Bayesian inference; Optimal experimental design; Nonlinear experimental design; Stochastic approximation; Shannon information; Chemical kinetics",
		"authors": [
			"Huan, X.",
			"Marzouk, Y.M."
		],
		"abstract": "<p>The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. We propose a general mathematical framework and an algorithmic approach for optimal experimental design with nonlinear simulation-based models; in particular, we focus on finding sets of experiments that provide the most information about targeted sets of parameters.<\/p><p>Our framework employs a Bayesian statistical setting, which provides a foundation for inference from noisy, indirect, and incomplete data, and a natural mechanism for incorporating heterogeneous sources of information. An objective function is constructed from information theoretic measures, reflecting expected information gain from proposed combinations of experiments. Polynomial chaos approximations and a two-stage Monte Carlo sampling method are used to evaluate the expected information gain. Stochastic approximation algorithms are then used to make optimization feasible in computationally intensive and high-dimensional settings. These algorithms are demonstrated on model problems and on nonlinear parameter inference problems arising in detailed combustion kinetics.<\/p>",
		"order": "21",
		"fulltext": "http:\/\/arxiv.org\/abs\/1108.4146",
		"journal": "Journal of Computational Physics",
		"doi": "10.1016\/j.jcp.2012.08.013",
		"year": "2013",
		"arxiv": "1108.4146",
		"volume": "232",
		"thumbnail": "jcp.png",
		"number": "1",
		"month": "1",
		"pages": "288\u2013317",
		"comments": "test"
	},
	{
		"id": "18",
		"title": "Bayesian inference with optimal maps",
		"keywords": "Bayesian inference; Optimal transport; Measure-preserving maps; Inverse problems; Polynomial chaos; Numerical optimization",
		"authors": [
			"Moselhy, T.A.",
			"Marzouk, Y.M."
		],
		"abstract": "<p>We present a new approach to Bayesian inference that entirely avoids Markov chain simulation, by constructing a map that pushes forward the prior measure to the posterior measure. Existence and uniqueness of a suitable measure-preserving map is established by formulating the problem in the context of optimal transport theory. We discuss various means of explicitly parameterizing the map and computing it efficiently through solution of an optimization problem, exploiting gradient information from the forward model when possible. The resulting algorithm overcomes many of the computational bottlenecks associated with Markov chain Monte Carlo. Advantages of a map-based representation of the posterior include analytical expressions for posterior moments and the ability to generate arbitrary numbers of independent posterior samples without additional likelihood evaluations or forward solves. The optimization approach also provides clear convergence criteria for posterior approximation and facilitates model selection through automatic evaluation of the marginal likelihood. We demonstrate the accuracy and efficiency of the approach on nonlinear inverse problems of varying dimension, involving the inference of parameters appearing in ordinary and partial differential equations.<\/p>",
		"order": "20",
		"fulltext": "http:\/\/arxiv.org\/abs\/1109.1516",
		"journal": "Journal of Computational Physics",
		"doi": "10.1016\/j.jcp.2012.07.022",
		"year": "2012",
		"arxiv": "",
		"volume": "231",
		"thumbnail": "jcp.png",
		"number": "23",
		"month": "0",
		"pages": "7815\u20137850",
		"comments": ""
	},
	{
		"id": "21",
		"title": "Bayesian reconstruction of binary media with unresolved fine-scale spatial structures",
		"keywords": "Upscaling; Binary media; Bayesian technique; Multiscale inference",
		"authors": [
			"Ray, J.",
			"McKenna, S.",
			"van Bloemen Waanders, B.",
			"Marzouk, Y.M."
		],
		"abstract": "",
		"order": "19",
		"fulltext": "http:\/\/dx.doi.org\/10.1016\/j.advwatres.2012.04.009",
		"journal": "Advances in Water Resources",
		"doi": "10.1016\/j.advwatres.2012.04.009",
		"year": "2012",
		"arxiv": "",
		"volume": "44",
		"thumbnail": "advances-water.png",
		"number": "",
		"month": "8",
		"pages": "1\u201319",
		"comments": ""
	},
	{
		"id": "20",
		"title": "Sequential data assimilation with multiple models",
		"keywords": "Uncertainty quantification; Data assimilation; Kalman filter; Model averaging",
		"authors": [
			"Narayan, A.",
			"Marzouk, Y.M.",
			"Xiu, D."
		],
		"abstract": "Data assimilation is an essential tool for predicting the behavior of real physical systems given approximate simulation models and limited observations. For many complex systems, there may exist several models, each with different properties and predictive capabilities. It is desirable to incorporate multiple models into the assimilation procedure in order to obtain a more accurate prediction of the physics than any model alone can provide. In this paper, we propose a framework for conducting sequential data assimilation with multiple models and sources of data. The assimilated solution is a linear combination of all model predictions and data. One notable feature is that the combination takes the most general form with matrix weights. By doing so the method can readily utilize different weights in different sections of the solution state vectors, allow the models and data to have different dimensions, and deal with the case of a singular state covariance. We prove that the proposed assimilation method, termed direct assimilation, minimizes a variational functional, a generalized version of the one used in the classical Kalman filter. We also propose an efficient iterative assimilation method that assimilates two models at a time until all models and data are assimilated. The mathematical equivalence of the iterative method and the direct method is established. Numerical examples are presented to demonstrate the effectiveness of the new method.",
		"order": "18",
		"fulltext": "http:\/\/dx.doi.org\/10.1016\/j.jcp.2012.06.002",
		"journal": "Journal of Computational Physics",
		"doi": "10.1016\/j.jcp.2012.06.002",
		"year": "2012",
		"arxiv": "",
		"volume": "231",
		"thumbnail": "jcp.png",
		"number": "19",
		"month": "0",
		"pages": "6401\u20136418",
		"comments": ""
	},
	{
		"id": "16",
		"title": "Data-free inference of the joint distribution of uncertain model parameters",
		"keywords": "Uncertainty quantification; Bayesian statistics; Missing information",
		"authors": [
			"Berry, R.D.",
			"Najm, H.N.",
			"Debusschere, B.J.",
			"Marzouk, Y.M.",
			"Adalsteinsson, H."
		],
		"abstract": "<p>A critical problem in accurately estimating uncertainty in model predictions is the lack of details in the literature on the correlation (or full joint distribution) of uncertain model parameters. In this paper we describe a framework and a class of algorithms for analyzing such \u201cmissing data\u201d problems in the setting of Bayesian statistics. The analysis focuses on the family of posterior distributions consistent with given statistics (e.g. nominal values, confidence intervals). The combining of consistent distributions is addressed via techniques from the opinion pooling literature. The developed approach allows subsequent propagation of uncertainty in model inputs consistent with reported statistics, in the absence of data.<\/p>",
		"order": "17",
		"fulltext": "http:\/\/dx.doi.org\/10.1016\/j.jcp.2011.10.031",
		"journal": "Journal of Computational Physics",
		"doi": "10.1016\/j.jcp.2011.10.031",
		"year": "2012",
		"arxiv": "",
		"volume": "231",
		"thumbnail": "jcp.png",
		"number": "5",
		"month": "0",
		"pages": "2180-2198",
		"comments": ""
	},
	{
		"id": "15",
		"title": "Computational singular perturbation with non-parametric tabulation of slow manifolds for time integration of stiff chemical kinetics",
		"keywords": "chemical kinetics, computational singular perturbation, slow manifold, non-parametric regression, nearest neighbors, kd-trees",
		"authors": [
			"Debusschere, B.J.",
			"Marzouk, Y.M.",
			"Najm, H.N.",
			"Rhoads, B.",
			"Goussis, D.A.",
			"Valorani, M."
		],
		"abstract": "<p>This paper presents a novel tabulation strategy for the adaptive numerical integration of chemical kinetics using the computational singular perturbation (CSP) method. The strategy stores and reuses CSP quantities required to filter out fast dissipative processes, resulting in a non-stiff chemical source term. In particular, non-parametric regression on low-dimensional slow invariant manifolds (SIMs) in the chemical state space is used to approximate the CSP vectors spanning the fast chemical subspace and the associated fast chemical time-scales. The relevant manifold and its dimension varies depending on the local number of exhausted modes at every location in the chemical state space. Multiple manifolds are therefore tabulated, corresponding to different numbers of exhausted modes (dimensions) and associated radical species. Non-parametric representations are inherently adaptive, and rely on efficient approximate-nearest-neighbor queries. As the CSP information is only a function of the non-radical species in the system and has relatively small gradients in the chemical state space, tabulation occurs in a lower-dimensional state space and at a relatively coarse level, thereby improving scalability to larger chemical mechanisms. The approach is demonstrated on the simulation of homogeneous constant pressure H2\u2013air and CH4\u2013air ignition, over a range of initial conditions. For CH4\u2013air, results are shown that outperform direct implicit integration of the stiff chemical kinetics while maintaining good accuracy.<\/p>",
		"order": "16",
		"fulltext": "http:\/\/dx.doi.org\/10.1080\/13647830.2011.596575",
		"journal": "Combustion Theory and Modeling",
		"doi": "10.1080\/13647830.2011.596575",
		"year": "2011",
		"arxiv": "",
		"volume": "16",
		"thumbnail": "tctm16.png",
		"number": "1",
		"month": "0",
		"pages": "173-198",
		"comments": ""
	},
	{
		"id": "14",
		"title": "Truncated multi-Gaussian fields and effective conductance of binary media",
		"keywords": "Upscaling; Binary media; Effective conductivity",
		"authors": [
			"McKenna, S.",
			"Ray, J.",
			"Marzouk, Y.M.",
			"van Bloemen Waanders, B."
		],
		"abstract": "<p>Truncated Gaussian fields provide a flexible model for defining binary media with dispersed (as opposed to layered) inclusions. General properties of excursion sets on these truncated fields are coupled with a distance-based upscaling algorithm and approximations of point process theory to develop an estimation approach for effective conductivity in two-dimensions. Estimation of effective conductivity is derived directly from knowledge of the kernel size used to create the multiGaussian field, defined as the full-width at half maximum (FWHM), the truncation threshold and conductance values of the two modes. Therefore, instantiation of the multiGaussian field is not necessary for estimation of the effective conductance. The critical component of the effective medium approximation developed here is the mean distance between high conductivity inclusions. This mean distance is characterized as a function of the FWHM, the truncation threshold and the ratio of the two modal conductivities. Sensitivity of the resulting effective conductivity to this mean distance is examined for two levels of contrast in the modal conductances and different FWHM sizes. Results demonstrate that the FWHM is a robust measure of mean travel distance in the background medium. The resulting effective conductivities are accurate when compared to numerical results and results obtained from effective media theory, distance-based upscaling and numerical simulation.<\/p>",
		"order": "15",
		"fulltext": "http:\/\/dx.doi.org\/10.1016\/j.advwatres.2011.02.011",
		"journal": "Advances in Water Resources",
		"doi": "10.1016\/j.advwatres.2011.02.011",
		"year": "2011",
		"arxiv": "",
		"volume": "34",
		"thumbnail": "advances-water.png",
		"number": "5",
		"month": "5",
		"pages": "617-626",
		"comments": ""
	},
	{
		"id": "13",
		"order": "14",
		"title": "Contributions of the wall boundary layer to the formation of the counter-rotating vortex pair in transverse jets.",
		"area": "",
		"authors": [
			"Schlegel, F.",
			"Wee, D.",
			"Marzouk, Y.M.",
			"Ghoniem, A.F."
		],
		"year": "2011",
		"journal": "Journal of Fluid Mechanics",
		"thumbnail": "jfm676.png",
		"volume": "676",
		"number": "1",
		"pages": "461-490",
		"month": "June",
		"abstract": "<p>Using high-resolution 3-D vortex simulations, this study seeks a mechanistic understanding of vorticity dynamics in transverse jets at a finite Reynolds number. A full no-slip boundary condition, rigorously formulated in terms of vorticity generation along the channel wall, captures unsteady interactions between the wall boundary layer and the jet \u2013 in particular, the separation of the wall boundary layer and its transport into the interior. For comparison, we also implement a reduced boundary condition that suppresses the separation of the wall boundary layer away from the jet nozzle. By contrasting results obtained with these two boundary conditions, we characterize near-field vortical structures formed as the wall boundary layer separates on the backside of the jet. Using various Eulerian and Lagrangian diagnostics, it is demonstrated that several near-wall vortical structures are formed as the wall boundary layer separates. The counter-rotating vortex pair, manifested by the presence of vortices aligned with the jet trajectory, is initiated closer to the jet exit. Moreover tornado-like wall-normal vortices originate from the separation of spanwise vorticity in the wall boundary layer at the side of the jet and from the entrainment of streamwise wall vortices in the recirculation zone on the lee side. These tornado-like vortices are absent in the case where separation is suppressed. Tornado-like vortices merge with counter-rotating vorticity originating in the jet shear layer, significantly increasing wall-normal circulation and causing deeper jet penetration into the crossflow stream.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1017\/jfm.2011.59",
		"doi": "10.1017\/jfm.2011.59",
		"keywords": "jets; vortex flows"
	},
	{
		"id": "12",
		"order": "13",
		"title": "Bayesian inference of atomic diffusivity in a binary Ni\/Al system based on molecular dynamics",
		"area": "",
		"authors": [
			"Rizzi, F.",
			"Salloum, M.",
			"Marzouk, Y.M.",
			"Xu, R.",
			"Falk, M.L.",
			"Weihs, T.P.",
			"Fritz, G.",
			"Knio, O.M."
		],
		"year": "2011",
		"journal": "SIAM Multiscale Modeling and Simulation",
		"thumbnail": "mms.png",
		"volume": "9",
		"number": "1",
		"pages": "486-512",
		"month": "March",
		"abstract": "<p>This work focuses on characterizing the integral features of atomic diffusion in Ni\/Al nanolaminates based on molecular dynamics (MD) computations. Attention is focused on the simplified problem of extracting the diffusivity, D, in an isothermal system at high temperature. To this end, a mixing measure theory is developed that relies on analyzing the moments of the cumulative distribution functions (CDFs) of the constituents. The mixing measures obtained from replica simulations are exploited in a Bayesian inference framework, based on contrasting these measures with corresponding moments of a dimensionless concentration evolving according to a Fickian process. The noise inherent in the MD simulations is described as a Gaussian process, and this hypothesis is verified both a priori and using a posterior predictive check. Computed values of D for an initially unmixed system rapidly heated to 1500 K are found to be consistent with experimental correlation for diffusion of Ni into molten Al. On the contrary, large discrepancies with experimental predictions are observed when D is estimated based on large-time mean-square displacement (MSD) analysis, and when it is evaluated using the Arrhenius correlation calibrated against experimental measurements of self-propagating front velocities. Implications are finally drawn regarding extension of the present work and potential refinement of continuum modeling approaches.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1137\/10080590X",
		"doi": "10.1137\/10080590X"
	},
	{
		"id": "11",
		"order": "12",
		"title": "A Bayesian approach for estimating bioterror attacks from patient data",
		"area": "",
		"authors": [
			"Ray, J.",
			"Marzouk, Y.M.",
			"Najm, H.N."
		],
		"year": "2010",
		"journal": "Statistics in Medicine",
		"thumbnail": "stat-med.png",
		"volume": "30",
		"number": "2",
		"pages": "101-126",
		"month": "October",
		"abstract": "<p>Terrorist attacks using an aerosolized pathogen have gained credibility as a national security concern after the anthrax attacks of 2001. Inferring some important details of the attack quickly, for example, the number of people infected, the time of infection, and a representative dose received can be crucial to planning a medical response. We use a Bayesian approach, based on a short time series of diagnosed patients, to estimate a joint probability density for these parameters. We first test the formulation with idealized cases and then apply it to realistic scenarios, including the Sverdlovsk anthrax outbreak of 1979. We also use simulated outbreaks to explore the impact of model error, as when the model used for generating simulated epidemic curves does not match the model subsequently used to characterize the attack. We find that in all cases except for the smallest attacks (fewer than 100 infected people), 3\u20135 days of data are sufficient to characterize the outbreak to a specificity that is useful for directing an emergency response.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1002\/sim.4090",
		"doi": "10.1002\/sim.4090",
		"keywords": "Bayesian inference;anthrax;Sverdlovsk outbreak;bioterrorism"
	},
	{
		"id": "10",
		"order": "11",
		"title": "Convergence Characteristics and Computational Cost of Two Algebraic Kernels in Vortex Methods with a Tree-Code Algorithm",
		"area": "",
		"authors": [
			"Wee, D.",
			"Marzouk, Y.M.",
			"Schlegel, F.",
			"Ghoniem, A.F."
		],
		"year": "2009",
		"journal": "SIAM Journal on Scientific Computing",
		"thumbnail": "sisc2.png",
		"volume": "31",
		"number": "4",
		"pages": "2510-2527",
		"month": "June",
		"abstract": "<p>We study the convergence characteristics of two algebraic kernels used in vortex calculations: the Rosenhead\u2013Moore kernel, which is a low-order kernel, and the Winckelmans\u2013Leonard kernel, which is a high-order kernel. To facilitate the study, a method of evaluating particle-cluster interactions is introduced for the Winckelmans\u2013Leonard kernel. The method is based on Taylor series expansion in Cartesian coordinates, as initially proposed by Lindsay and Krasny [J. Comput. Phys., 172 (2001), pp. 879\u2013907] for the Rosenhead\u2013Moore kernel. A recurrence relation for the Taylor coefficients of the Winckelmans\u2013Leonard kernel is derived by separating the kernel into two parts, and an error estimate is obtained to ensure adaptive error control. The recurrence relation is incorporated into a tree-code to evaluate vorticity-induced velocity. Next, comparison of convergence is made while utilizing the tree-code. Both algebraic kernels lead to convergence, but the Winckelmans\u2013Leonard kernel exhibits a superior convergence rate. The combined desingularization and discretization error from the Winckelmans\u2013Leonard kernel is an order of magnitude smaller than that from the Rosenhead\u2013Moore kernel at a typical resolution. Simulations of vortex rings are performed using the two algebraic kernels in order to compare their performance in a practical setting. In particular, numerical simulations of the side-by-side collision of two identical vortex rings suggest that the three-dimensional evolution of vorticity at finite resolution can be greatly affected by the choice of the kernel. We find that the Winckelmans\u2013Leonard kernel is able to perform the same task with a much smaller number of vortex elements than the Rosenhead\u2013Moore kernel, greatly reducing the overall computational cost.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1137\/080726872",
		"doi": "10.1137\/080726872"
	},
	{
		"id": "9",
		"order": "10",
		"title": "Dimensionality reduction and polynomial chaos acceleration of Bayesian inference in inverse problems",
		"area": "",
		"authors": [
			"Marzouk, Y.M.",
			"Najm, H.N."
		],
		"year": "2009",
		"journal": "Journal of Computational Physics",
		"thumbnail": "jcp.png",
		"volume": "228",
		"number": "6",
		"pages": "1862-1902",
		"month": "April",
		"abstract": "<p>We consider a Bayesian approach to nonlinear inverse problems in which the unknown quantity is a spatial or temporal field, endowed with a hierarchical Gaussian process prior. Computational challenges in this construction arise from the need for repeated evaluations of the forward model (e.g., in the context of Markov chain Monte Carlo) and are compounded by high dimensionality of the posterior. We address these challenges by introducing truncated Karhunen\u2013Lo\u00e8ve expansions, based on the prior distribution, to efficiently parameterize the unknown field and to specify a stochastic forward problem whose solution captures that of the deterministic forward model over the support of the prior. We seek a solution of this problem using Galerkin projection on a polynomial chaos basis, and use the solution to construct a reduced-dimensionality surrogate posterior density that is inexpensive to evaluate. We demonstrate the formulation on a transient diffusion equation with prescribed source terms, inferring the spatially-varying diffusivity of the medium from limited and noisy data.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1016\/j.jcp.2008.11.024",
		"doi": "10.1016\/j.jcp.2008.11.024",
		"keywords": "Inverse problems; Bayesian inference; Dimensionality reduction; Polynomial chaos; Markov chain Monte Carlo; Galerkin projection; Gaussian processes; Karhunen\u2013Lo\u00e8ve expansion; RKHS"
	},
	{
		"id": "8",
		"order": "9",
		"title": "A stochastic collocation approach to Bayesian inference in inverse problems",
		"area": "",
		"authors": [
			"Marzouk, Y.M.",
			"Xiu, D.B."
		],
		"year": "2009",
		"journal": "Communications in Computational Physics",
		"thumbnail": "ccp.png",
		"volume": "6",
		"number": "1",
		"pages": "826-847",
		"month": "December",
		"abstract": "<p>We present an efficient numerical strategy for the Bayesian solution of inverse problems. Stochastic collocation methods, based on generalized polynomial chaos (gPC), are used to construct a polynomial approximation of the forward solution over the support of the prior distribution. This approximation then defines a surrogate posterior probability density that can be evaluated repeatedly at minimal computational cost. The ability to simulate a large number of samples from the posterior distribution results in very accurate estimates of the inverse solution and its associated uncertainty. Combined with high accuracy of the gPC-based forward solver, the new algorithm can provide great efficiency in practical applications. A rigorous error analysis of the algorithm is conducted, where we establish convergence of the approximate posterior to the true posterior and obtain an estimate of the convergence rate. It is proved that fast (exponential) convergence of the gPC forward solution yields similarly fast (exponential) convergence of the posterior. The numerical strategy and the predicted convergence rates are then demonstrated on nonlinear inverse problems ofvarying smoothness and dimension.<\/p>",
		"fulltext": "http:\/\/docs.lib.purdue.edu\/prism\/16\/",
		"doi": "DOI:prism\/16"
	},
	{
		"id": "7",
		"order": "8",
		"title": "Uncertainty quantification in chemical systems",
		"area": "",
		"authors": [
			"Najm, H.N.",
			"Debusschere, B.J.",
			"Marzouk, Y.M.",
			"Widmer, S.",
			"LeMa\u00eetre, O."
		],
		"year": "2009",
		"journal": "International Journal for Numerical Methods in Engineering",
		"thumbnail": "ijnme.png",
		"volume": "80",
		"number": "6-7",
		"pages": "789-814",
		"month": "November",
		"abstract": "<p>We demonstrate the use of multiwavelet spectral polynomial chaos techniques for uncertainty quantification in non-isothermal ignition of a methane\u2013air system. We employ Bayesian inference for identifying the probabilistic representation of the uncertain parameters and propagate this uncertainty through the ignition process. We analyze the time evolution of moments and probability density functions of the solution. We also examine the role and significance of dependence among the uncertain parameters. We finish with a discussion of the role of non-linearity and the performance of the algorithm.<\/p>",
		"fulltext": "http:\/\/onlinelibrary.wiley.com\/doi\/10.1002\/nme.2551\/abstract",
		"doi": "10.1002\/nme.2551",
		"keywords": "uncertainty quantification;polynomial chaos;multiwavelet;chemistry;ignition"
	},
	{
		"id": "6",
		"order": "7",
		"title": "Bayesian Inference of Spectral Expansions for Predictability Assessment in Stochastic Reaction Networks",
		"area": "",
		"authors": [
			"Sargsyan, K.",
			"Debusschere, B.J.",
			"Najm, H.N.",
			"Marzouk, Y.M."
		],
		"year": "2009",
		"journal": "Journal of Computational and Theoretical Nanoscience",
		"thumbnail": "jcompnano.png",
		"volume": "6",
		"number": "10",
		"pages": "2283-2297",
		"month": "October",
		"abstract": "<p>Stochastic reaction networks modeled as jump Markov processes serve as the main mathematical representation of biochemical phenomena in cells, particularly when the relevant molecule count is low, causing deterministic macroscale chemical reaction models to fail. Further, as there is mainly empirical knowledge about the rate parameters, parametric uncertainty analysis becomes very important. The conventional predictability tools for deterministic systems do not readily generalize to the stochastic setting. We use spectral polynomial chaos expansions to represent stochastic processes. Bayesian inference techniques with Markov chain Monte Carlo are used to find the best spectral representation of the system state, taking into account not only intrinsic stochastic noise but also parametric uncertainties. A likelihood-based adaptive domain decomposition is introduced and applied, in particular, for the cases when the parameter range includes deterministic bifurcations. We show that the adaptive multidomain polynomial chaos representation captures the correct system behavior for a benchmark bistable Schl\u00f6gl model for a wide range of parameter variations.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1166\/jctn.2009.1285",
		"doi": "10.1166\/jctn.2009.1285",
		"keywords": "uncertainty quantification; bayesian inference; polynomial chaos; stochastic reaction networks; domain decomposition; predictability"
	},
	{
		"id": "5",
		"order": "6",
		"title": "Stochastic spectral methods for efficient Bayesian solution of inverse problems",
		"area": "",
		"authors": [
			"Marzouk, Y.M.",
			"Najm, H.N.",
			"Rahn, L.A."
		],
		"year": "2007",
		"journal": "Journal of Computational Physics",
		"thumbnail": "jcp.png",
		"volume": "224",
		"number": "2",
		"pages": "560-586",
		"month": "June",
		"abstract": "<p>We present a reformulation of the Bayesian approach to inverse problems, that seeks to accelerate Bayesian inference by using polynomial chaos (PC) expansions to represent random variables. Evaluation of integrals over the unknown parameter space is recast, more efficiently, as Monte Carlo sampling of the random variables underlying the PC expansion. We evaluate the utility of this technique on a transient diffusion problem arising in contaminant source inversion. The accuracy of posterior estimates is examined with respect to the order of the PC representation, the choice of PC basis, and the decomposition of the support of the prior. The computational cost of the new scheme shows significant gains over direct sampling.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1016\/j.jcp.2006.10.010",
		"keywords": "Inverse problems; Bayesian inference; Polynomial chaos; Monte Carlo; Markov chain Monte Carlo; Spectral methods; Galerkin projection; Diffusive transport",
		"doi": "10.1016\/j.jcp.2006.10.010"
	},
	{
		"id": "4",
		"order": "5",
		"title": "Vorticity structure and evolution in a transverse jet",
		"area": "",
		"authors": [
			"Marzouk, Y.M.",
			"Ghoniem, A.F."
		],
		"year": "2007",
		"journal": "Journal of Fluid Mechanics",
		"thumbnail": "jfm575.png",
		"volume": "575",
		"number": "1",
		"pages": "267-305",
		"month": "March",
		"abstract": "<p>Transverse jets arise in many applications, including propulsion, effluent dispersion, oil field flows, and V\/STOL aerodynamics. This study seeks a fundamental, mechanistic understanding of the structure and evolution of vorticity in the transverse jet. We develop a high-resolution three-dimensional vortex simulation of the transverse jet at large Reynolds number and consider jet-to-crossflow velocity ratios r ranging from 5 to 10. A new formulation of vorticity-flux boundary conditions accounts for the interaction of channel wall vorticity with the jet flow immediately around the orifice. We demonstrate that the nascent jet shear layer contains not only azimuthal vorticity generated in the jet pipe, but wall-normal and azimuthal perturbations resulting from the jet\u2013crossflow interaction. This formulation also yields analytical expressions for vortex lines in the near field as a function of $r$.<\/p><p>Transformation of the cylindrical shear layer emanating from the orifice begins with axial elongation of its lee side to form sections of counter-rotating vorticity aligned with the jet trajectory. Periodic roll-up of the shear layer accompanies this deformation, creating complementary vortex arcs on the lee and windward sides of the jet. Counter-rotating vorticity then drives lee-side roll-ups in the windward direction, along the normal to the jet trajectory. Azimuthal vortex arcs of alternating sign thus approach each other on the windward boundary of the jet. Accordingly, initially planar material rings on the shear layer fold completely and assume an interlocking structure that persists for several diameters above the jet exit. Though the near field of the jet is dominated by deformation and periodic roll-up of the shear layer, the resulting counter-rotating vorticity is a pronounced feature of the mean field; in turn, the mean counter-rotation exerts a substantial influence on the deformation of the shear layer. Following the pronounced bending of the trajectory into the crossflow, we observe a sudden breakdown of near-field vortical structures into a dense distribution of smaller scales. Spatial filtering of this region reveals the persistence of counter-rotating streamwise vorticity initiated in the near field.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1017\/S0022112006004411",
		"doi": "10.1017\/S0022112006004411"
	},
	{
		"id": "3",
		"order": "4",
		"title": "K-means clustering for optimal partitioning and dynamic load balancing of parallel hierarchical N-body simulations",
		"area": "",
		"authors": [
			"Marzouk, Y.M.",
			"Ghoniem, A.F."
		],
		"year": "2005",
		"journal": "Journal of Computational Physics",
		"volume": "207",
		"thumbnail": "jcp.png",
		"number": "2",
		"pages": "493-528",
		"month": "August",
		"abstract": "<p>A number of complex physical problems can be approached through N-body simulation, from fluid flow at high Reynolds number to gravitational astrophysics and molecular dynamics. In all these applications, direct summation is prohibitively expensive for large N and thus hierarchical methods are employed for fast summation. This work introduces new algorithms, based on k-means clustering, for partitioning parallel hierarchical N-body interactions. We demonstrate that the number of particle\u2013cluster interactions and the order at which they are performed are directly affected by partition geometry. Weighted k-means partitions minimize the sum of clusters\u2019 second moments and create well-localized domains, and thus reduce the computational cost of N-body approximations by enabling the use of lower-order approximations and fewer cells.<\/p><p>We also introduce compatible techniques for dynamic load balancing, including adaptive scaling of cluster volumes and adaptive redistribution of cluster centroids. We demonstrate the performance of these algorithms by constructing a parallel treecode for vortex particle simulations, based on the serial variable-order Cartesian code developed by Lindsay and Krasny [Journal of Computational Physics 172 (2) (2001) 879\u2013907]. The method is applied to vortex simulations of a transverse jet. Results show outstanding parallel efficiencies even at high concurrencies, with velocity evaluation errors maintained at or below their serial values; on a realistic distribution of 1.2 million vortex particles, we observe a parallel efficiency of 98% on 1024 processors. Excellent load balance is achieved even in the face of several obstacles, such as an irregular, time-evolving particle distribution containing a range of length scales and the continual introduction of new vortex particles throughout the domain. Moreover, results suggest that k-means yields a more efficient partition of the domain than a global oct-tree.<\/p>",
		"fulltext": "http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0021999105000380",
		"doi": "10.1016\/j.jcp.2005.01.021",
		"keywords": "k-means clustering; Treecode; N-body problems; Hierarchical methods; Parallel processing; Load balancing; Particle methods; Vortex methods; Three-dimensional flow; Transverse jet"
	},
	{
		"id": "2",
		"order": "3",
		"title": "Toward a flame embedding model for turbulent combustion simulation",
		"area": "",
		"authors": [
			"Marzouk, Y.M.",
			"Ghoniem, A.F.",
			"Najm, H.N."
		],
		"year": "2003",
		"journal": "AIAA Journal",
		"thumbnail": "aiaa.png",
		"volume": "41",
		"number": "4",
		"pages": "641-652",
		"publisher": "American Institute of Aeronautics and Astronautics",
		"abstract": "<p>Combustion in turbulent flows may take the form of a thin flame wrapped around vortical structures. For this regime, the flame embedding approach seeks to decouple computations of the outer nonreacting flow and the combustion zone by discretizing the flame surface into a number of elemental flames, each incorporating the local impact of unsteady flow-flame interaction. An unsteady strained laminar flame solver, based on a boundary-layer approximation of combustion in a time-dependent stagnation-point potential flow, is proposed as an elemental flame model. To validate the concept, two-dimensional simulations of premixed flame-vortex interactions are performed for a matrix of vortex strengths and length scales, and a section of the flame is selected for comparison with the flame embedding model results. Results show that using the flame leading-edge strain rate gives reasonable agreement in the cases of low strain rate and weak strain rate gradient within the flame structure. This agreement deteriorates substantially when both are high. We propose two different schemes, both based on averaging the strain rate across the flame structure, and demonstrate that agreement between the one-dimensional model and the two-dimensional simulation greatly improves when the actual strain rate at the reaction zone of the one-dimensional flame is made to match that of the two-dimensional flame.<\/p>",
		"fulltext": "http:\/\/www.refdoc.fr\/Detailnotice?idarticle=8912185",
		"doi": "",
		"keywords": "Two dimensional model; One dimensional model; Premixed flame; Finite difference method; Numerical simulation; Flame structure; Turbulent flame; Combustion"
	},
	{
		"id": "1",
		"order": "2",
		"title": "Dynamic response of strained premixed flames to equivalence ratio gradients",
		"area": "",
		"authors": [
			"Marzouk, Y.M.",
			"Ghoniem, A.F.",
			"Najm, H.N."
		],
		"year": "2000",
		"journal": "Proceedings of the Combustion Institute",
		"thumbnail": "proceedingscombinst.png",
		"volume": "28",
		"number": "2",
		"pages": "1859-1866",
		"month": "",
		"abstract": "<p>Premixed flames encounter gradients of mixture equivalence ratio in stratified charge engines, lean premixed gas-turbine engines, and a variety of other applications. In cases for which the scales\u2014spatial or temporal\u2014of fuel concentration gradients in the reactants are comparable to flame scales, changes in burning rate, flammability limits, and flame structure have been observed. This paper uses an unsteady strained flame in the stagnation point configuration to examine the effect of temporal gradients on combustion in a premixed methane\/air mixture. An inexact Newton backtracking method, coupled with a preconditioned Krylov subspace iterative solver, was used to improve the efficiency of the numerical solution and expand its domain of convergence in the presence of detailed chemistry.<\/p><p>Results indicate that equivalence ratio variations with timescales lower than 10 ms have significant effects on the burning process, including reaction zone broadening, burning rate enhancement, and extension of the flammability limit toward learner mixtures. While the temperature of a flame processing a stoichiometric-to-lean equivalence ratio gradient decreased slightly within the front side of the reaction zone, radical concentrations remained elevated over the entire flame structure. These characteristics are linked to a feature reminiscent of \u201cback-supported\u201d flames\u2014flames in which a stream of products resulting from burning at higher equivalence ratio is continuously supplied to lower equivalence ratio reactants. The relevant feature is the establishment of a positive temperature gradient on the products side of the flame which maintains the temperature high enough and the radical concentration sufficient to sustain combustion there. Unsteadiness in equivalence ratio produces similar gradients within the flame structure, thus compensating for the change in temperature at the leading edge of the reaction zone and accounting for an observed \u201cflame inertia\u201d. For sufficiently large equivalence ratio gradients, a flame starting in a stoichiometric mixture can burn through a very lean one by taking advantage of this mechanism.<\/p>",
		"fulltext": "http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0082078400805895",
		"doi": "10.1016\/S0082-0784(00)80589-5"
	},
	{
		"id": "0",
		"order": "1",
		"title": "Asymmetric autocorrelation function to resolve directional ambiguity in PIV images",
		"area": "",
		"authors": [
			"Marzouk, Y.M.",
			"Hart, D.P."
		],
		"year": "1998",
		"journal": "Experiments in Fluids",
		"thumbnail": "experiments-in-fluids.png",
		"publisher": "Springer-Verlag",
		"volume": "25",
		"number": "5-6",
		"pages": "401-408",
		"month": "October",
		"abstract": "<p>Autocorrelation of a double-exposed image, unlike cross-correlation between two images, produces a correlation function that is symmetric about the origin. Thus, while it is possible to calculate the speed and direction of tracer particles in a particle image velocimetry (PIV) image using autocorrelation, it is impossible to tell whether the velocity is in the positive or negative direction. This ambiguity can be resolved by spatially shifting one exposure relative to the next or labeling exposures with color or polarization, but the complexity and limitations of these methods can be prohibitive. It is, however, possible to resolve the sign of the velocity from a triple-exposed image using unequal time intervals between exposures.<\/p><p>Triple-exposed images, like double-exposed images, correlate symmetrically about zero. The directional ambiguity, however, can be resolved by calculating the probability that the three exposures occur in a specific temporal order; that is, by assuming that the correlation has a specific sign and testing to see if the assumption is correct. Traditional spectral and statistical correlation techniques are unable to accomplish this. Presented herein is a computationally efficient asymmetric correlation function that is able to differentiate the temporal order of triple exposed images. Included is a discussion of the limitations of this function and of difficulties in experimental implementation.<\/p>",
		"fulltext": "http:\/\/dx.doi.org\/10.1007\/s003480050247",
		"doi": "10.1007\/s003480050247"
	}
]